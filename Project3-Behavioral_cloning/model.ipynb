{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/likangning/anaconda/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation, Flatten, SpatialDropout2D, ELU\n",
    "from keras.layers import Convolution2D,MaxPooling2D,Cropping2D\n",
    "from keras.layers.core import Lambda\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###load data\n",
    "##read from csv and store the lines from driveing_log.csv file\n",
    "##then for each line, extract the path to the camera image\n",
    "lines= []\n",
    "def read_lines(driving_log_path, extract_lines):\n",
    "    with open(driving_log_path) as csvfile:\n",
    "        reader=csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            extract_lines.append(line)\n",
    "    return extract_lines\n",
    "lines =read_lines(\"/Users/likangning/Desktop/data/driving_log.csv\",lines)\n",
    "#print(\"lines\",len(lines))\n",
    "lines=lines[1:]\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples,validation_samples =train_test_split(lines,test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=train_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_2016_12_01_13_35_20_965.jpg\n"
     ]
    }
   ],
   "source": [
    "print(x[0].split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7634 402 8036\n"
     ]
    }
   ],
   "source": [
    "print(len(train_samples),len(validation_samples),len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Preprocessing data...\n",
    "###original input shape is (160,320,3)\n",
    "###1.resize.. width=200\n",
    "###2.crop..   heigth=66\n",
    "###3.Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_imgSize=(66,200,3)\n",
    "def resize1(z):\n",
    "    height=160\n",
    "    width=320\n",
    "    \n",
    "    factor=float(new_imgSize[1]/float(width))\n",
    "    \n",
    "    resized_size=(int(width*factor),int(height*factor))\n",
    "    \n",
    "    z=cv2.resize(z,resized_size)\n",
    "    \n",
    "    crop_height= resized_size[1]-new_imgSize[0]\n",
    "    \n",
    "    return z[crop_height:,:,:]\n",
    "\n",
    "def rgb_to_yuv(x):    ####ACCORDING TO NVIDIA \n",
    "    return cv2.cvtColor(x,cv2.COLOR_RGB2YUV)\n",
    "    \n",
    "def normalize(image):\n",
    "    ####Normalize the input between [-0.5,0.5]\n",
    "    return image /255. -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_correction=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(lines,batch_size=32):\n",
    "    num_lines=len(lines)\n",
    "    while 1: ## Loop forever so the generator never terminates\n",
    "        shuffle(lines)\n",
    "        for offset in range(0,num_lines,batch_size):\n",
    "            batch_lines= lines[offset:offset+batch_size]\n",
    "            images=[]\n",
    "            angles=[]\n",
    "            for batch_line in batch_lines:\n",
    "                #name=\"/Users/likangning/Desktop/data/IMG/\"+batch_line[0].split(\"/\")[-1]\n",
    "                #name=\"/Users/likangning/Desktop/raw_data/\"+batch_line[0]\n",
    "                #name=batch_line[0]\n",
    "                #center_image =cv2.imread(name)\n",
    "                #center_image = cv2.cvtColor(center_image, cv2.COLOR_BGR2RGB)\n",
    "                center_angle= float(batch_line[3])\n",
    "                left_angle= center_angle + angle_correction\n",
    "                right_angle= center_angle - angle_correction\n",
    "                \n",
    "                for i in range(3):\n",
    "                    source_path=batch_line[i]\n",
    "                    file_name= source_path.split(\"/\")[-1]\n",
    "                    current_path=\"/Users/likangning/Desktop/data/IMG/\" +file_name\n",
    "                    image_bgr= cv2.imread(current_path)\n",
    "                    image = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "                    images.append(image)\n",
    "                    images.append(cv2.flip(image,1))\n",
    "                  \n",
    "                #images.append(center_image)\n",
    "                angles.extend([center_angle,center_angle*-1,\n",
    "                               left_angle,left_angle*-1,\n",
    "                               right_angle,right_angle*-1])\n",
    "              \n",
    "        ###trim image to only see section with road\n",
    "        ###convert the image to np.array\n",
    "                x_train= np.array(images)\n",
    "                y_train= np.array(angles)\n",
    "        ###yield in generator means return in normal function\n",
    "                yield shuffle(x_train,y_train)\n",
    "\n",
    "        \n",
    "### Compile and train the model using the generator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(samples,sample_size,mode):\n",
    "    num_samples=len(samples)\n",
    "    if mode== \"train\":\n",
    "        cameras=[\"center\",\"left\",\"right\"]\n",
    "    else:\n",
    "        cameras=[\"center\"]\n",
    "        \n",
    "    ##generator loop\n",
    "    \n",
    "    while 1:\n",
    "        sklearnrnrnearnearnearn.utils.shuffle(samples,random_state=43)\n",
    "        for offnum_samples_samples in range(0, num_samples,sample_size):\n",
    "            batch_sample=samples[offset:offset+sample_size]\n",
    "            \n",
    "            images=[]\n",
    "            angles=[]\n",
    "            \n",
    "            for sample in batch_samples:\n",
    "                for cam in cameras:\n",
    "                    if mode==\"train\":\n",
    "                        augmentation=np.random.choice([\"flipping\",\"brightness\",\"shift\",\"none\"])\n",
    "                    else:\n",
    "                        augmentation=\"none\"\n",
    "                    \n",
    "                    image= None\n",
    "                    angle= float(sample[3])\n",
    "                    \n",
    "                    if cam==\"center\":\n",
    "                        image=cv2.imread(\"/Users/likangning/Desktop/data/\"+sample[0])\n",
    "                    elif cam==\"left\":\n",
    "                        image=cv2.imread(\"/Users/likangning/Desktop/data/\"+sample[1])\n",
    "                        angle+=0.2\n",
    "                    elif cam==\"right\":\n",
    "                        image=cv2.imread(\"/Users/likangning/Desktop/data/\"+sample[2])\n",
    "                        angle-=0.2\n",
    "                        \n",
    "                        \n",
    "                    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "                    if augmentation==\"flipping\":\n",
    "                        image=cv2.flip(image,1)\n",
    "                        angle*=-1.0\n",
    "                    elif augmentation==\"brightness\":\n",
    "                        image= cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "                        image=np.array(image,dtype=np.float64)\n",
    "                        brightness= np.random.uniform()+0.5\n",
    "                        image[:,:,2]=image[:,:,2]*brightness\n",
    "                        image[:,:,2][image[:,:,2]>255]=255\n",
    "                        image=np.array(image,dtype=np.float64)\n",
    "                        image= cv2.cvtColor(image,cv2.COLOR_HSV2RGB)\n",
    "                        \n",
    "                    elif augmentation==\"shift\":\n",
    "                        tran_x=np.random.randint(0,100) -50\n",
    "                        angle += tran_x*0.004\n",
    "                        trans_y=np.random.randint(0,40)-20\n",
    "                        \n",
    "                        trans_matrix=np.float32([[1,0,trans_x],[0,1,trans_y]])\n",
    "                        image=cv2.warpAffine(image,trans_matrix,(320,160))\n",
    "                        \n",
    "                    image= image[50:140,0:320]\n",
    "                    \n",
    "                    image=cv2.resize(image,(200,66),interpolation=cv2.INTER_AREA)\n",
    "                    images.append(np.reshape(image,(1, 66,200,3)))\n",
    "                    angles.append(np.array([[angle]]))\n",
    "                    \n",
    "                        \n",
    "            X_train=np.vstack(images)\n",
    "            y_train=np.vstack(angles)\n",
    "            yield sklearn.utils.shuffle(X_train,y_train,random_state=21)\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator= generator(train_samples,batch_size=32)\n",
    "validation_generator= generator(validation_samples,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object generator at 0x124381d00>\n"
     ]
    }
   ],
   "source": [
    "print(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image):\n",
    "    import tensorflow as tf\n",
    "    return tf.image.resize_images(image,(66,200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "cropping2d_1 (Cropping2D)        (None, 65, 320, 3)    0           cropping2d_input_1[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 66, 200, 3)    0           cropping2d_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 66, 200, 3)    0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 17, 50, 16)    3088        lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 17, 50, 16)    0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 9, 25, 32)     12832       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 9, 25, 32)     0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 5, 13, 64)     51264       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 5, 13, 64)     0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 5, 13, 64)     36928       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 5, 13, 64)     0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 4160)          0           activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4160)          0           flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 4160)          0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           2130432     activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 512)           0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 50)            25650       activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 50)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 1)             51          activation_7[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 2,260,245\n",
      "Trainable params: 2,260,245\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    model1= Sequential()\n",
    "    \n",
    "    model1.add(Cropping2D(cropping=((70,25),(0,0)),input_shape=(160,320,3)))\n",
    "    \n",
    "    model1.add(Lambda(resize))\n",
    "    model1.add(Lambda(normalize))\n",
    "    model1.add(Convolution2D(16,8,8,border_mode=\"same\",subsample=(4,4)))\n",
    "    #model1.add(MaxPooling2D(2,2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Convolution2D(32,5,5,border_mode=\"same\",subsample=(2,2)))\n",
    "    #model1.add(MaxPooling2D(2,2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Convolution2D(64,5,5,border_mode=\"same\",subsample=(2,2)))\n",
    "    #model1.add(MaxPooling2D(2,2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    \n",
    "    model1.add(Convolution2D(64,3,3,border_mode=\"same\",subsample=(1,1)))\n",
    "    #model1.add(MaxPooling2D(2,2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    \n",
    "    #model1.add(Convolution2D(64,3,3,border_mode=\"valid\",subsample=(1,1)))\n",
    "    #model1.add(MaxPooling2D(2,2))\n",
    "    #model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Flatten())\n",
    "    model1.add(Dropout(0.2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    \n",
    "    #model1.add(Dense(200))\n",
    "    #model1.add(Dropout(0.2))\n",
    "    #model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Dense(512))\n",
    "    model1.add(Dropout(0.5))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Dense(50))\n",
    "    #model1.add(Dropout(0.2))\n",
    "    model1.add(Activation(\"relu\"))\n",
    "    \n",
    "    model1.add(Dense(1))\n",
    "    \n",
    "    \n",
    "    model1.summary()\n",
    "    \n",
    "    model1.compile(loss=\"mse\",optimizer=Adam(lr= 0.001))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "nb_epoch=20\n",
    "\n",
    "checkpointer= ModelCheckpoint(filepath=\"./Checkpointer/comma-4c.{epoch:02d}-{val_loss:.2f}.hdf5\",verbose=1,save_best_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7596/7634 [============================>.] - ETA: 0s - loss: 0.0144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/likangning/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: saving model to ./Checkpointer/comma-4c.00-0.03.hdf5\n",
      "7722/7634 [==============================] - 56s - loss: 0.0143 - val_loss: 0.0310\n",
      "Epoch 2/20\n",
      "7560/7634 [============================>.] - ETA: 0s - loss: 0.0112Epoch 00001: saving model to ./Checkpointer/comma-4c.01-0.05.hdf5\n",
      "7740/7634 [==============================] - 53s - loss: 0.0111 - val_loss: 0.0461\n",
      "Epoch 3/20\n",
      "7632/7634 [============================>.] - ETA: 0s - loss: 0.0079Epoch 00002: saving model to ./Checkpointer/comma-4c.02-0.02.hdf5\n",
      "7740/7634 [==============================] - 52s - loss: 0.0080 - val_loss: 0.0246\n",
      "Epoch 4/20\n",
      "7578/7634 [============================>.] - ETA: 0s - loss: 0.0137Epoch 00003: saving model to ./Checkpointer/comma-4c.03-0.02.hdf5\n",
      "7746/7634 [==============================] - 51s - loss: 0.0138 - val_loss: 0.0236\n",
      "Epoch 5/20\n",
      "7614/7634 [============================>.] - ETA: 0s - loss: 0.0133Epoch 00004: saving model to ./Checkpointer/comma-4c.04-0.01.hdf5\n",
      "7698/7634 [==============================] - 52s - loss: 0.0133 - val_loss: 0.0112\n",
      "Epoch 6/20\n",
      "7506/7634 [============================>.] - ETA: 0s - loss: 0.0155Epoch 00005: saving model to ./Checkpointer/comma-4c.05-0.04.hdf5\n",
      "7656/7634 [==============================] - 54s - loss: 0.0154 - val_loss: 0.0404\n",
      "Epoch 7/20\n",
      "7614/7634 [============================>.] - ETA: 0s - loss: 0.0137Epoch 00006: saving model to ./Checkpointer/comma-4c.06-0.05.hdf5\n",
      "7644/7634 [==============================] - 55s - loss: 0.0137 - val_loss: 0.0547\n",
      "Epoch 8/20\n",
      "7632/7634 [============================>.] - ETA: 0s - loss: 0.0099Epoch 00007: saving model to ./Checkpointer/comma-4c.07-0.05.hdf5\n",
      "7764/7634 [==============================] - 56s - loss: 0.0099 - val_loss: 0.0465\n",
      "Epoch 9/20\n",
      "7608/7634 [============================>.] - ETA: 0s - loss: 0.0104Epoch 00008: saving model to ./Checkpointer/comma-4c.08-0.01.hdf5\n",
      "7794/7634 [==============================] - 53s - loss: 0.0103 - val_loss: 0.0126\n",
      "Epoch 10/20\n",
      "7554/7634 [============================>.] - ETA: 0s - loss: 0.0118Epoch 00009: saving model to ./Checkpointer/comma-4c.09-0.01.hdf5\n",
      "7668/7634 [==============================] - 55s - loss: 0.0118 - val_loss: 0.0132\n",
      "Epoch 11/20\n",
      "7632/7634 [============================>.] - ETA: 0s - loss: 0.0082Epoch 00010: saving model to ./Checkpointer/comma-4c.10-0.02.hdf5\n",
      "7806/7634 [==============================] - 54s - loss: 0.0082 - val_loss: 0.0214\n",
      "Epoch 12/20\n",
      "7614/7634 [============================>.] - ETA: 0s - loss: 0.0099Epoch 00011: saving model to ./Checkpointer/comma-4c.11-0.04.hdf5\n",
      "7710/7634 [==============================] - 54s - loss: 0.0102 - val_loss: 0.0420\n",
      "Epoch 13/20\n",
      "7626/7634 [============================>.] - ETA: 0s - loss: 0.0143Epoch 00012: saving model to ./Checkpointer/comma-4c.12-0.01.hdf5\n",
      "7788/7634 [==============================] - 57s - loss: 0.0143 - val_loss: 0.0083\n",
      "Epoch 14/20\n",
      "7632/7634 [============================>.] - ETA: 0s - loss: 0.0097Epoch 00013: saving model to ./Checkpointer/comma-4c.13-0.03.hdf5\n",
      "7704/7634 [==============================] - 53s - loss: 0.0101 - val_loss: 0.0299\n",
      "Epoch 15/20\n",
      "7524/7634 [============================>.] - ETA: 0s - loss: 0.0144Epoch 00014: saving model to ./Checkpointer/comma-4c.14-0.02.hdf5\n",
      "7668/7634 [==============================] - 53s - loss: 0.0142 - val_loss: 0.0207\n",
      "Epoch 16/20\n",
      "7512/7634 [============================>.] - ETA: 0s - loss: 0.0123Epoch 00015: saving model to ./Checkpointer/comma-4c.15-0.02.hdf5\n",
      "7704/7634 [==============================] - 55s - loss: 0.0122 - val_loss: 0.0207\n",
      "Epoch 17/20\n",
      "7596/7634 [============================>.] - ETA: 0s - loss: 0.0105Epoch 00016: saving model to ./Checkpointer/comma-4c.16-0.01.hdf5\n",
      "7722/7634 [==============================] - 54s - loss: 0.0106 - val_loss: 0.0142\n",
      "Epoch 18/20\n",
      "7560/7634 [============================>.] - ETA: 0s - loss: 0.0122Epoch 00017: saving model to ./Checkpointer/comma-4c.17-0.02.hdf5\n",
      "7740/7634 [==============================] - 55s - loss: 0.0122 - val_loss: 0.0174\n",
      "Epoch 19/20\n",
      "7632/7634 [============================>.] - ETA: 0s - loss: 0.0106Epoch 00018: saving model to ./Checkpointer/comma-4c.18-0.02.hdf5\n",
      "7740/7634 [==============================] - 54s - loss: 0.0107 - val_loss: 0.0180\n",
      "Epoch 20/20\n",
      "7578/7634 [============================>.] - ETA: 0s - loss: 0.0153Epoch 00019: saving model to ./Checkpointer/comma-4c.19-0.02.hdf5\n",
      "7746/7634 [==============================] - 51s - loss: 0.0152 - val_loss: 0.0163\n",
      "Training done...\n"
     ]
    }
   ],
   "source": [
    "model1.fit_generator(train_generator,\n",
    "                     samples_per_epoch=len(train_samples),\n",
    "                     validation_data=validation_generator,\n",
    "                     nb_val_samples=len(validation_samples),\n",
    "                     nb_epoch=nb_epoch,\n",
    "                     callbacks=[checkpointer],\n",
    "                     verbose=1)\n",
    "\n",
    "print(\"Training done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model.json..\n"
     ]
    }
   ],
   "source": [
    "model_json=model1.to_json()\n",
    "with open(\"model.json\",\"w\") as json_file:\n",
    "     json_file.write(model_json)\n",
    "print(\"saving model.json..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "model1.save(\"model.h5\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
